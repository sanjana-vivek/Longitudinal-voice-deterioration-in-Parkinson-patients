{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4ZZN1jdy0aPYCi2eTWf/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjana-vivek/Longitudinal-voice-deterioration-in-Parkinson-s-patients-/blob/main/Novel_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upgrade basics\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "\n",
        "# Core audio + processing\n",
        "!pip install librosa soundfile matplotlib pandas tqdm scipy scikit-learn\n",
        "\n",
        "# Praat bindings\n",
        "!pip install praat-parselmouth\n",
        "\n",
        "# Nonlinear & recurrence\n",
        "!pip install nolds\n",
        "\n",
        "# Wavelet libraries\n",
        "!pip install PyWavelets\n",
        "\n",
        "# Wavelet scattering\n",
        "!pip install kymatio\n",
        "\n",
        "# Transformers & torch\n",
        "!pip install transformers torch\n",
        "\n",
        "# Optional: openSMILE wrapper\n",
        "# !pip install opensmile\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVFz9b3agLoD",
        "outputId": "cd3cb692-50d5-414f-bf71-22d5a6d46c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
            "Collecting praat-parselmouth\n",
            "  Downloading praat_parselmouth-0.4.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from praat-parselmouth) (2.0.2)\n",
            "Downloading praat_parselmouth-0.4.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: praat-parselmouth\n",
            "Successfully installed praat-parselmouth-0.4.6\n",
            "Collecting nolds\n",
            "  Downloading nolds-0.6.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>1.0 in /usr/local/lib/python3.12/dist-packages (from nolds) (2.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from nolds) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from nolds) (80.9.0)\n",
            "Downloading nolds-0.6.2-py2.py3-none-any.whl (225 kB)\n",
            "Installing collected packages: nolds\n",
            "Successfully installed nolds-0.6.2\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (1.9.0)\n",
            "Requirement already satisfied: numpy<3,>=1.25 in /usr/local/lib/python3.12/dist-packages (from PyWavelets) (2.0.2)\n",
            "Collecting kymatio\n",
            "  Downloading kymatio-0.3.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting appdirs (from kymatio)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting configparser (from kymatio)\n",
            "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from kymatio) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kymatio) (25.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from kymatio) (1.16.1)\n",
            "Downloading kymatio-0.3.0-py3-none-any.whl (87 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: appdirs, configparser, kymatio\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [kymatio]\n",
            "\u001b[1A\u001b[2KSuccessfully installed appdirs-1.4.4 configparser-7.2.0 kymatio-0.3.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import pywt\n",
        "import nolds\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.signal import resample, medfilt\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "\n",
        "#imports that might be heavy/fail - wrapping in try/except for graceful fallback\n",
        "try:\n",
        "    import parselmouth  # praat bindings\n",
        "    from parselmouth.praat import call as praat_call\n",
        "    PRAAT_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    print(\"parselmouth not available:\", e)\n",
        "    PRAAT_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import kymatio\n",
        "    from kymatio import Scattering1D\n",
        "    KYMATIO_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    print(\"kymatio not available:\", e)\n",
        "    KYMATIO_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except Exception as e:\n",
        "    print(\"transformers/torch import failed:\", e)\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "\n",
        "print(\"PRAAT_AVAILABLE:\", PRAAT_AVAILABLE, \"KYMATIO_AVAILABLE:\", KYMATIO_AVAILABLE, \"TRANSFORMERS_AVAILABLE:\", TRANSFORMERS_AVAILABLE)\n"
      ],
      "metadata": {
        "id": "0zvAbPXafmQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ceec1f-47a1-4539-cd5e-fedf8ebe11cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nolds/datasets.py:9: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRAAT_AVAILABLE: True KYMATIO_AVAILABLE: True TRANSFORMERS_AVAILABLE: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch / GPU environment check\n",
        "import torch\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "    print(\"Current device:\", torch.cuda.current_device())\n",
        "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print(\"Running on CPU only\")\n"
      ],
      "metadata": {
        "id": "OgOrbvRAfpQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c0aa30-2986-42cd-a9a0-1768416afdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.8.0+cu126\n",
            "CUDA available: False\n",
            "Running on CPU only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6pydKLmeFj5",
        "outputId": "55d664ce-ef73-4663-b842-bba10c279d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "\n",
        "BASE_PATH = \"/content/drive/My Drive/pd_data/denoised-speech-dataset\"\n",
        "\n",
        "# Look recursively for wav files (case-insensitive)\n",
        "wav_files = glob.glob(os.path.join(BASE_PATH, \"**\", \"*.wav\"), recursive=True)\n",
        "wav_files += glob.glob(os.path.join(BASE_PATH, \"**\", \"*.WAV\"), recursive=True)\n",
        "\n",
        "print(f\"Total audio files found: {len(wav_files)}\")\n",
        "print(\"Sample files:\", wav_files[:5])\n",
        "\n",
        "\n",
        "# import os\n",
        "# subfolder = os.path.join(BASE_PATH, \"emma\")\n",
        "# print(\"Files in emma:\", os.listdir(subfolder)[:10])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pX0uwwCXyXr",
        "outputId": "e92da50a-3723-4f75-bada-c0f10aded6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total audio files found: 578\n",
            "Sample files: ['/content/drive/My Drive/pd_data/denoised-speech-dataset/LW/LW20.wav', '/content/drive/My Drive/pd_data/denoised-speech-dataset/LW/LW13.wav', '/content/drive/My Drive/pd_data/denoised-speech-dataset/LW/LW10.wav', '/content/drive/My Drive/pd_data/denoised-speech-dataset/LW/LW15.wav', '/content/drive/My Drive/pd_data/denoised-speech-dataset/LW/LW1.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyroomacoustics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q77luslqFQ_j",
        "outputId": "a228f63c-b807-4d56-e7b8-7a91c2a5a881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyroomacoustics in /usr/local/lib/python3.12/dist-packages (0.8.4)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (from pyroomacoustics) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from pyroomacoustics) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.12/dist-packages (from pyroomacoustics) (1.16.1)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.12/dist-packages (from pyroomacoustics) (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic audio helpers and file discovery according to your structure\n",
        "BASE_DIR = \"/content/drive/MyDrive/pd_data/denoised-speech-dataset\"\n",
        "\n",
        "TARGET_SR = 16000  # wav2vec requirement and standardize\n",
        "\n",
        "def list_all_wavs(base_dir=BASE_DIR):\n",
        "    pattern = os.path.join(base_dir, \"**\", \"*.wav\")\n",
        "    files = sorted(glob.glob(pattern, recursive=True))\n",
        "    return files\n",
        "\n",
        "def load_resample(path, sr=TARGET_SR):\n",
        "    # read with soundfile to preserve bit depth, then resample if needed\n",
        "    y, orig_sr = sf.read(path, always_2d=False)\n",
        "    if y.ndim > 1:\n",
        "        y = np.mean(y, axis=1)  # to mono\n",
        "    if orig_sr != sr:\n",
        "        y = librosa.resample(y.astype(np.float32), orig_sr, sr)\n",
        "    return y.astype(np.float32), sr\n",
        "\n",
        "def vad_trim(y, sr=TARGET_SR, top_db=25):\n",
        "    # simple energy-based trim using librosa\n",
        "    intervals = librosa.effects.split(y, top_db=top_db)\n",
        "    if len(intervals) == 0:\n",
        "        return y  # nothing to trim\n",
        "    trimmed = np.concatenate([y[s:e] for s,e in intervals])\n",
        "    return trimmed\n",
        "\n",
        "# Discover files for sanity check\n",
        "all_wavs = list_all_wavs()\n",
        "print(f\"Found {len(all_wavs)} .wav files under {BASE_DIR}. Example (first 10):\")\n",
        "for p in all_wavs[:10]:\n",
        "    print(\" \", p)\n",
        "print(\"Cell 4 finished: helper functions ready and dataset scanned.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbl-jhweJABj",
        "outputId": "9a0b3831-36dd-4f1d-beb1-b487a1b18aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 578 .wav files under /content/drive/MyDrive/pd_data/denoised-speech-dataset. Example (first 10):\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL1.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL10.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL16.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL17.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL18.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL19.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL2.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL21.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL22.wav\n",
            "  /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL23.wav\n",
            "Cell 4 finished: helper functions ready and dataset scanned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SPL frame RMS -> dB computations and session stats\n",
        "def framewise_rms_db(y, sr=TARGET_SR, frame_ms=25, hop_ms=10, amin=1e-9):\n",
        "    frame_len = int(sr * frame_ms / 1000)\n",
        "    hop_len = int(sr * hop_ms / 1000)\n",
        "    if len(y) < frame_len:\n",
        "        # pad short signals\n",
        "        pad = frame_len - len(y)\n",
        "        y = np.pad(y, (0, pad))\n",
        "    frames = librosa.util.frame(y, frame_length=frame_len, hop_length=hop_len).astype(np.float32)\n",
        "    rms = np.sqrt(np.mean(frames**2, axis=0) + amin)\n",
        "    db = 20 * np.log10(rms + amin)  # relative dB (not calibrated SPL)\n",
        "    return db\n",
        "\n",
        "def session_spl_stats(y, sr=TARGET_SR):\n",
        "    db = framewise_rms_db(y, sr)\n",
        "    stats = {\n",
        "        \"spl_mean_db\": float(np.mean(db)),\n",
        "        \"spl_peak_db\": float(np.max(db)),\n",
        "        \"spl_std_db\": float(np.std(db)),\n",
        "        \"spl_frames\": int(len(db))\n",
        "    }\n",
        "    print(f\"[SPL] mean={stats['spl_mean_db']:.2f} dB, peak={stats['spl_peak_db']:.2f} dB, frames={stats['spl_frames']}\")\n",
        "    return stats\n",
        "\n",
        "# Demo on a sample file (if present)\n",
        "if len(all_wavs) > 0:\n",
        "    path = all_wavs[0]\n",
        "    y, sr = load_resample(path)\n",
        "    y_trim = vad_trim(y, sr)\n",
        "    print(\"File used for demo:\", path)\n",
        "    spl_demo_raw = session_spl_stats(y, sr)\n",
        "    spl_demo_trim = session_spl_stats(y_trim, sr)\n",
        "else:\n",
        "    print(\"No wav files found to demo SPL calculations.\")\n",
        "print(\"Cell 5 finished: SPL calculation functions available (printed demo stats).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0pju6kBKRVZ",
        "outputId": "281bb4f7-4d86-46c4-a39b-8c43f3437691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File used for demo: /content/drive/MyDrive/pd_data/denoised-speech-dataset/DL/DL1.wav\n",
            "[SPL] mean=-53.47 dB, peak=-22.08 dB, frames=898\n",
            "[SPL] mean=-36.69 dB, peak=-21.60 dB, frames=523\n",
            "Cell 5 finished: SPL calculation functions available (printed demo stats).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# Optional: use parselmouth if available\n",
        "try:\n",
        "    import parselmouth\n",
        "    PRAAT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PRAAT_AVAILABLE = False\n",
        "    print(\"Parselmouth not available. Fallback to librosa for F0 only.\")\n",
        "\n",
        "# ------------------------------\n",
        "# Helper functions (adjust as needed)\n",
        "# ------------------------------\n",
        "def load_resample(path, target_sr=16000):\n",
        "    y, sr = librosa.load(path, sr=None)\n",
        "    if sr != target_sr:\n",
        "        y = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
        "        sr = target_sr\n",
        "    return y, sr\n",
        "\n",
        "def vad_trim(y, sr):\n",
        "    # Simple energy-based VAD trim\n",
        "    y_trim, _ = librosa.effects.trim(y, top_db=30)\n",
        "    return y_trim\n",
        "\n",
        "# ------------------------------\n",
        "# Classical feature extraction\n",
        "# ------------------------------\n",
        "def classical_features(path):\n",
        "    y, sr = load_resample(path)\n",
        "    y_trim = vad_trim(y, sr)\n",
        "    feats = {}\n",
        "\n",
        "    if PRAAT_AVAILABLE:\n",
        "        try:\n",
        "            snd = parselmouth.Sound(y_trim, sr)\n",
        "\n",
        "            # --- Pitch (F0 statistics)\n",
        "            try:\n",
        "                pitch = snd.to_pitch()\n",
        "                f0_values = pitch.selected_array['frequency']\n",
        "                f0_values = f0_values[f0_values > 0]\n",
        "                feats['f0_median'] = float(np.median(f0_values)) if len(f0_values) > 0 else np.nan\n",
        "                feats['f0_std'] = float(np.std(f0_values)) if len(f0_values) > 0 else np.nan\n",
        "            except Exception as e:\n",
        "                feats['f0_median'] = np.nan\n",
        "                feats['f0_std'] = np.nan\n",
        "                print(\"Pitch extraction error:\", e)\n",
        "\n",
        "            # --- Jitter & Shimmer\n",
        "            try:\n",
        "                pp = parselmouth.praat.call(snd, \"To PointProcess (periodic, cc)\", 75, 500)\n",
        "                feats['jitter_local'] = float(parselmouth.praat.call(pp, \"Get jitter (local)\", 0, 0, 75, 500, 1.3))\n",
        "                feats['shimmer_local'] = float(parselmouth.praat.call(pp, \"Get shimmer (local)\", 0, 0, 75, 500, 1.3, 1.6, 0.03, 1.6))\n",
        "            except Exception as e:\n",
        "                feats['jitter_local'] = np.nan\n",
        "                feats['shimmer_local'] = np.nan\n",
        "                print(\"Jitter/Shimmer error:\", e)\n",
        "\n",
        "            # --- HNR\n",
        "            try:\n",
        "                ham = parselmouth.praat.call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1)\n",
        "                feats['hnr_mean'] = float(parselmouth.praat.call(ham, \"Get mean\", 0, 0))\n",
        "            except Exception as e:\n",
        "                feats['hnr_mean'] = np.nan\n",
        "                print(\"HNR error:\", e)\n",
        "\n",
        "            # --- Formants F1-F3\n",
        "            try:\n",
        "                form = snd.to_formant_burg()\n",
        "                def form_mean(idx):\n",
        "                    times = np.linspace(0, snd.duration, num=50)\n",
        "                    vals = []\n",
        "                    for t in times:\n",
        "                        v = parselmouth.praat.call(form, \"Get value at time\", idx, t, \"Hertz\", \"Linear\")\n",
        "                        if v > 0: vals.append(v)\n",
        "                    return float(np.mean(vals)) if len(vals) > 0 else np.nan\n",
        "                feats['f1_mean'] = form_mean(1)\n",
        "                feats['f2_mean'] = form_mean(2)\n",
        "                feats['f3_mean'] = form_mean(3)\n",
        "            except Exception as e:\n",
        "                feats['f1_mean'] = np.nan\n",
        "                feats['f2_mean'] = np.nan\n",
        "                feats['f3_mean'] = np.nan\n",
        "                print(\"Formants error:\", e)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Parselmouth failed for file:\", path, e)\n",
        "            # mark all as NaN\n",
        "            for key in ['f0_median','f0_std','jitter_local','shimmer_local','hnr_mean','f1_mean','f2_mean','f3_mean']:\n",
        "                feats[key] = np.nan\n",
        "\n",
        "    else:\n",
        "        # --- Fallback: librosa pitch estimation (pyin)\n",
        "        try:\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(y_trim, fmin=50, fmax=500, sr=sr)\n",
        "            f0_vals = f0[~np.isnan(f0)]\n",
        "            feats['f0_median'] = float(np.median(f0_vals)) if len(f0_vals) > 0 else np.nan\n",
        "            feats['f0_std'] = float(np.std(f0_vals)) if len(f0_vals) > 0 else np.nan\n",
        "        except Exception as e:\n",
        "            feats['f0_median'] = np.nan\n",
        "            feats['f0_std'] = np.nan\n",
        "            print(\"librosa pyin failed:\", e)\n",
        "        # Other classical features cannot be reliably computed without Praat\n",
        "        for key in ['jitter_local','shimmer_local','hnr_mean','f1_mean','f2_mean','f3_mean']:\n",
        "            feats[key] = np.nan\n",
        "        print(\"Praat not available: classical features partially computed via librosa where possible.\")\n",
        "\n",
        "    print(f\"[Classical] Extracted features for: {os.path.basename(path)}\")\n",
        "    return feats\n",
        "\n",
        "# ------------------------------\n",
        "# Demo on first WAV\n",
        "# ------------------------------\n",
        "if len(all_wavs) > 0:\n",
        "    sample_file = all_wavs[0]\n",
        "    cf = classical_features(sample_file)\n",
        "    print(\"Sample classical features:\\n\", cf)\n",
        "else:\n",
        "    print(\"No WAV files found for demo.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ASH4APo4OyEo",
        "outputId": "1445d181-d10c-4472-b902-e3d2d2dc9c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_wavs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3154700382.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Demo on first WAV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;31m# ------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_wavs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0msample_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_wavs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0mcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassical_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_wavs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-linear and complexity features\n",
        "from scipy.stats import entropy\n",
        "import nolds\n",
        "\n",
        "def spectral_entropy(y, sr=TARGET_SR, n_fft=2048, hop_length=512):\n",
        "    S = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))**2\n",
        "    ps = S / (np.sum(S, axis=0, keepdims=True) + 1e-12)\n",
        "    ent = np.mean([entropy(col + 1e-12) for col in ps.T])\n",
        "    return float(ent)\n",
        "\n",
        "def permutation_entropy(y, order=3, delay=1):\n",
        "    x = y.copy()\n",
        "    if len(x) > 10000:\n",
        "        x = librosa.resample(x, orig_sr=TARGET_SR, target_sr=1000)\n",
        "    n = len(x)\n",
        "    patterns = {}\n",
        "    for i in range(n - (order-1)*delay):\n",
        "        window = x[i:i+order*delay:delay]\n",
        "        ranks = tuple(np.argsort(window))\n",
        "        patterns[ranks] = patterns.get(ranks, 0) + 1\n",
        "    ps = np.array(list(patterns.values()), dtype=float)\n",
        "    ps = ps / (ps.sum() + 1e-12)\n",
        "    return float(-np.sum(ps * np.log2(ps + 1e-12)))\n",
        "\n",
        "def recurrence_rate(y, emb_dim=3, tau=1, eps_factor=0.1):\n",
        "    N = len(y)\n",
        "    M = N - (emb_dim-1)*tau\n",
        "    if M <= 0:\n",
        "        return np.nan\n",
        "    X = np.zeros((M, emb_dim))\n",
        "    for i in range(M):\n",
        "        for j in range(emb_dim):\n",
        "            X[i, j] = y[i + j * tau]\n",
        "    X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-9)\n",
        "    dists = np.sqrt(((X[:, None, :] - X[None, :, :])**2).sum(axis=2))\n",
        "    eps = eps_factor * np.median(dists)\n",
        "    rr = np.sum(dists < eps) / (M*M)\n",
        "    return float(rr)\n",
        "\n",
        "def rpde_approx(y):\n",
        "    # approximate predictability via short-window AR residuals (simple)\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    y = (y - np.mean(y)) / (np.std(y) + 1e-9)\n",
        "    win = int(0.5 * TARGET_SR)\n",
        "    step = int(0.25 * TARGET_SR)\n",
        "    errs = []\n",
        "    for s in range(0, max(1, len(y)-win), step):\n",
        "        seg = y[s:s+win]\n",
        "        if len(seg) < 50:\n",
        "            continue\n",
        "        order = 4\n",
        "        if len(seg) < order + 1:\n",
        "            continue\n",
        "        X = np.vstack([seg[i: i+len(seg)-order] for i in range(order)]).T\n",
        "        ytarget = seg[order:]\n",
        "        try:\n",
        "            lr = LinearRegression().fit(X, ytarget)\n",
        "            pred = lr.predict(X)\n",
        "            errs.append(np.mean((pred - ytarget)**2))\n",
        "        except:\n",
        "            pass\n",
        "    return float(np.mean(errs)) if len(errs)>0 else np.nan\n",
        "\n",
        "def nonlinear_features(path):\n",
        "    y, sr = load_resample(path)\n",
        "    y_trim = vad_trim(y, sr)\n",
        "    feats = {\n",
        "        \"spectral_entropy\": spectral_entropy(y_trim, sr),\n",
        "        \"perm_entropy\": permutation_entropy(y_trim),\n",
        "        \"recurrence_rate\": recurrence_rate(y_trim),\n",
        "        \"rpde_approx\": rpde_approx(y_trim)\n",
        "    }\n",
        "    print(f\"[NonLinear] computed for {os.path.basename(path)}: entropy={feats['spectral_entropy']:.3f}, perm_ent={feats['perm_entropy']:.3f}\")\n",
        "    return feats\n",
        "\n",
        "# Demo\n",
        "if len(all_wavs)>0:\n",
        "    print(nonlinear_features(all_wavs[0]))\n",
        "else:\n",
        "    print(\"No files for non-linear demo.\")\n",
        "print(\"Processing finished: non-linear features functions ready.\")\n"
      ],
      "metadata": {
        "id": "KSGODTxhO1Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wavelet band energies (pywt) and scattering (kymatio)\n",
        "def wavelet_band_energy(y, wavelet='db4', levels=5):\n",
        "    coeffs = pywt.wavedec(y, wavelet, level=levels)\n",
        "    energies = [float(np.sum(c**2)) for c in coeffs]\n",
        "    total = sum(energies) + 1e-12\n",
        "    ratios = [e/total for e in energies]\n",
        "    feats = {f\"wavelet_energy_{i}\": energies[i] for i in range(len(energies))}\n",
        "    feats.update({f\"wavelet_ratio_{i}\": ratios[i] for i in range(len(ratios))})\n",
        "    print(f\"[Wavelet] computed {len(energies)} band energies\")\n",
        "    return feats\n",
        "\n",
        "def wavelet_scattering_feats(y):\n",
        "    if not KYMATIO_AVAILABLE:\n",
        "        print(\"kymatio not available: skipping scattering features.\")\n",
        "        return {}\n",
        "    # Kymatio requires length; pad/truncate to next power of two length for stability\n",
        "    N = len(y)\n",
        "    L = 1 << (N-1).bit_length()\n",
        "    if L < N:\n",
        "        L <<= 1\n",
        "    y_pad = np.zeros(L, dtype=np.float32)\n",
        "    y_pad[:N] = y\n",
        "    scattering = Scattering1D(J=6, shape=L, Q=8)\n",
        "    import torch\n",
        "    Sx = scattering(torch.from_numpy(y_pad).float().unsqueeze(0))\n",
        "    Sx_np = Sx.squeeze(0).numpy()\n",
        "    feats = {}\n",
        "    # pool mean/std across time dimension\n",
        "    if Sx_np.ndim == 2:\n",
        "        for i in range(Sx_np.shape[0]):\n",
        "            feats[f\"scat_mean_{i}\"] = float(Sx_np[i].mean())\n",
        "            feats[f\"scat_std_{i}\"] = float(Sx_np[i].std())\n",
        "    print(f\"[Scattering] produced {len(feats)//2} coefficients (mean+std)\")\n",
        "    return feats\n",
        "\n",
        "# Demo\n",
        "if len(all_wavs)>0:\n",
        "    y, sr = load_resample(all_wavs[0])\n",
        "    print(wavelet_band_energy(y))\n",
        "    print(wavelet_scattering_feats(y))\n",
        "else:\n",
        "    print(\"No files for wavelet demo.\")\n",
        "print(\"Processing finished: wavelet features ready (scattering optional).\")\n"
      ],
      "metadata": {
        "id": "tcCDSd2bPmnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wav2vec embeddings summary + baseline-drift\n",
        "# Note: this uses HuggingFace Wav2Vec2 via transformers. Ensure transformers and torch are installed.\n",
        "\n",
        "if TRANSFORMERS_AVAILABLE:\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    model.eval()\n",
        "else:\n",
        "    processor = None\n",
        "    model = None\n",
        "\n",
        "def extract_wav2vec_stats(y, sr=TARGET_SR):\n",
        "    if not TRANSFORMERS_AVAILABLE:\n",
        "        print(\"transformers not available: skipping embedding extraction.\")\n",
        "        return {}\n",
        "    # processor expects numpy array, sampling rate TARGET_SR\n",
        "    import torch\n",
        "    input_values = processor(y, sampling_rate=sr, return_tensors=\"pt\", padding=True).input_values\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_values)\n",
        "    emb = outputs.last_hidden_state.squeeze(0).cpu().numpy()  # frames x dim\n",
        "    stats = {\n",
        "        \"emb_mean_mean\": float(np.mean(emb.mean(axis=0))),\n",
        "        \"emb_mean_std\": float(np.std(emb.mean(axis=0))),\n",
        "        \"emb_flat_skew\": float(skew(emb.flatten())),\n",
        "        \"emb_flat_kurtosis\": float(kurtosis(emb.flatten())),\n",
        "        \"emb_frames\": int(emb.shape[0])\n",
        "    }\n",
        "    print(f\"[Embeddings] frames={stats['emb_frames']}, mean_mean={stats['emb_mean_mean']:.4f}\")\n",
        "    return stats\n",
        "\n",
        "def embedding_baseline_drift(y_emb, baseline_emb):\n",
        "    # y_emb, baseline_emb are arrays frames x dim (or mean vectors)\n",
        "    if y_emb is None or baseline_emb is None:\n",
        "        return {\"embedding_drift\": np.nan}\n",
        "    # reduce to mean vectors if frames x dim\n",
        "    y_mean = np.mean(y_emb, axis=0) if y_emb.ndim==2 else y_emb\n",
        "    b_mean = np.mean(baseline_emb, axis=0) if baseline_emb.ndim==2 else baseline_emb\n",
        "    # cosine distance\n",
        "    cos_sim = 1 - cdist([y_mean], [b_mean], metric='cosine')[0,0]\n",
        "    drift = float(1 - cos_sim)  # distance measure\n",
        "    return {\"embedding_drift\": drift}\n",
        "\n",
        "# Demo\n",
        "if len(all_wavs)>0 and TRANSFORMERS_AVAILABLE:\n",
        "    y, sr = load_resample(all_wavs[0])\n",
        "    emb_stats = extract_wav2vec_stats(y, sr)\n",
        "    print(emb_stats)\n",
        "else:\n",
        "    print(\"No embedding demo (either no files or transformers not available).\")\n",
        "print(\"Cell 9 finished: embedding feature functions ready (baseline-drift function included).\")\n"
      ],
      "metadata": {
        "id": "no5p_jbsPu7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full per-patient pipeline that extracts all features, computes baseline normalization,\n",
        "# per-feature slope, a simple deterioration_score and CUSUM detection; then plots deterioration line.\n",
        "\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import datetime\n",
        "\n",
        "def extract_features_for_file(path):\n",
        "    \"\"\"Extract all features for a single WAV file and return dict\"\"\"\n",
        "    feats = {}\n",
        "    y, sr = load_resample(path)\n",
        "    y_trim = vad_trim(y, sr)\n",
        "    # SPL\n",
        "    feats.update(session_spl_stats(y_trim, sr))\n",
        "    # Classical\n",
        "    feats.update(classical_features(path))\n",
        "    # Nonlinear\n",
        "    feats.update(nonlinear_features(path))\n",
        "    # Wavelet\n",
        "    feats.update(wavelet_band_energy(y_trim))\n",
        "    # Scattering\n",
        "    feats.update(wavelet_scattering_feats(y_trim))\n",
        "    # Embeddings stats\n",
        "    if TRANSFORMERS_AVAILABLE:\n",
        "        try:\n",
        "            emb_stats = extract_wav2vec_stats(y_trim, sr)\n",
        "            feats.update(emb_stats)\n",
        "        except Exception as e:\n",
        "            print(\"Embedding extraction failed for\", path, \":\", e)\n",
        "    # metadata\n",
        "    feats[\"_file\"] = path\n",
        "    # session date: try parse from text file next to wav (if exists) else file mtime\n",
        "    txt_path = os.path.splitext(path)[0] + \".txt\"\n",
        "    if os.path.exists(txt_path):\n",
        "        try:\n",
        "            with open(txt_path, 'r', encoding='utf-8') as fh:\n",
        "                txt = fh.read().strip()\n",
        "            feats[\"_transcript\"] = txt\n",
        "            # Optionally parse date from filename if encoded - here we fallback to mtime\n",
        "        except:\n",
        "            feats[\"_transcript\"] = \"\"\n",
        "    else:\n",
        "        feats[\"_transcript\"] = \"\"\n",
        "    feats[\"_date\"] = datetime.fromtimestamp(os.path.getmtime(path))\n",
        "    return feats\n",
        "\n",
        "def analyze_patient(patient_group, patient_subfolder=None):\n",
        "    \"\"\"Analyze a single top-level patient group (e.g., 'emma') OR a specific subfolder inside it\"\"\"\n",
        "    if patient_subfolder:\n",
        "        root_dir = os.path.join(BASE_DIR, patient_group, patient_subfolder)\n",
        "    else:\n",
        "        root_dir = os.path.join(BASE_DIR, patient_group)\n",
        "    wavs = sorted(glob.glob(os.path.join(root_dir, \"**\", \"*.wav\"), recursive=True))\n",
        "    if len(wavs) == 0:\n",
        "        print(\"No wavs found for\", root_dir); return None\n",
        "    print(f\"Found {len(wavs)} wav files for analysis under {root_dir}\")\n",
        "    rows = []\n",
        "    for w in tqdm(wavs):\n",
        "        try:\n",
        "            r = extract_features_for_file(w)\n",
        "            rows.append(r)\n",
        "        except Exception as e:\n",
        "            print(\"Error extracting\", w, e)\n",
        "    df = pd.DataFrame(rows)\n",
        "    # Sort by date if available\n",
        "    df = df.sort_values(\"_date\").reset_index(drop=True)\n",
        "    # Baseline normalization: baseline = mean of first N sessions (N=1 or 2)\n",
        "    baseline_N = 2 if len(df) >=2 else 1\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    baseline = df.loc[:baseline_N-1, numeric_cols].mean(skipna=True)\n",
        "    # compute normalized columns and slope per feature (slope per day)\n",
        "    slope_map = {}\n",
        "    for col in numeric_cols:\n",
        "        base = baseline.get(col, np.nan)\n",
        "        df[f\"{col}_norm\"] = (df[col] - base) / (abs(base) + 1e-9) if not np.isnan(base) else np.nan\n",
        "        # slope over time (value vs days)\n",
        "        if df.shape[0] >= 2:\n",
        "            x = np.array([(d - df[\"_date\"].iloc[0]).days for d in df[\"_date\"]]).reshape(-1,1)\n",
        "            yv = df[col].fillna(method='ffill').fillna(0).values.reshape(-1,1)\n",
        "            lr = LinearRegression().fit(x, yv)\n",
        "            slope_map[col] = float(lr.coef_[0][0])\n",
        "        else:\n",
        "            slope_map[col] = np.nan\n",
        "    # create a simple weighted deterioration_score using a few clinically important features\n",
        "    # weight signs chosen so that higher deterioration_score -> worse\n",
        "    weights = {\n",
        "        \"spl_mean_db\": -1.0,   # decreasing SPL (negative slope) is worse so negative weight\n",
        "        \"jitter_local\": 1.0,\n",
        "        \"shimmer_local\": 1.0,\n",
        "        \"rpde_approx\": 1.0\n",
        "    }\n",
        "    # compute weighted sum of slopes\n",
        "    weighted_sum = 0.0\n",
        "    for feat, w in weights.items():\n",
        "        s = slope_map.get(feat, 0.0)\n",
        "        if np.isnan(s): s = 0.0\n",
        "        weighted_sum += w * s\n",
        "    # attach deterioration score per session as cumulative slope proxy (we show the aggregate slope as single line)\n",
        "    # For plotting over sessions, we compute per-session deterioration_score as weighted sum of normalized features\n",
        "    def per_session_score(row):\n",
        "        s = 0.0\n",
        "        for feat, w in weights.items():\n",
        "            ncol = f\"{feat}_norm\"\n",
        "            if ncol in row and not np.isnan(row[ncol]):\n",
        "                s += w * row[ncol]\n",
        "        return s\n",
        "    df[\"deterioration_score\"] = df.apply(per_session_score, axis=1)\n",
        "    # CUSUM on deterioration_score\n",
        "    def simple_cusum(series, threshold=2.0, drift=0.0):\n",
        "        pos = 0.0; neg = 0.0; alarms = []\n",
        "        mu = np.nanmean(series); sigma = np.nanstd(series) + 1e-9\n",
        "        for x in series:\n",
        "            s = (x - mu) / sigma if sigma>0 else 0.0\n",
        "            pos = max(0, pos + s - drift)\n",
        "            neg = min(0, neg + s + drift)\n",
        "            if pos > threshold:\n",
        "                alarms.append(\"pos_alarm\")\n",
        "            elif abs(neg) > threshold:\n",
        "                alarms.append(\"neg_alarm\")\n",
        "            else:\n",
        "                alarms.append(\"ok\")\n",
        "        return alarms\n",
        "    df[\"cusum_flag\"] = simple_cusum(df[\"deterioration_score\"].fillna(0).values)\n",
        "    # Plot deterioration line (per-session score) with baseline ribbon\n",
        "    plt.figure(figsize=(10,4))\n",
        "    dates = df[\"_date\"]\n",
        "    scores = df[\"deterioration_score\"]\n",
        "    plt.plot(dates, scores, marker='o', label=\"deterioration_score\")\n",
        "    # baseline ribbon: +- 2*std of baseline sessions (if available)\n",
        "    baseline_vals = df.loc[:baseline_N-1, \"deterioration_score\"]\n",
        "    if not baseline_vals.empty:\n",
        "        bmean = baseline_vals.mean(); bstd = baseline_vals.std()\n",
        "        plt.fill_between(dates, bmean - 2*bstd, bmean + 2*bstd, color='gray', alpha=0.2, label=\"baseline ± 2σ\")\n",
        "    plt.title(f\"Deterioration score over time: {patient_group} / {patient_subfolder if patient_subfolder else ''}\")\n",
        "    plt.ylabel(\"Deterioration score (higher = worse)\")\n",
        "    plt.xticks(rotation=25)\n",
        "    plt.grid(True); plt.legend()\n",
        "    plt.show()\n",
        "    # Print summary\n",
        "    print(\"Feature slopes (per day) sample (subset):\")\n",
        "    sample_slopes = {k: slope_map[k] for k in list(slope_map.keys())[:10]}\n",
        "    print(sample_slopes)\n",
        "    print(\"CUSUM flags (last 10 sessions):\", df[\"cusum_flag\"].tail(10).tolist())\n",
        "    print(\"Cell 10 finished: patient analysis plotted and summary printed.\")\n",
        "    return df, slope_map\n",
        "\n",
        "# Example usage for patient 'emma' top-level group (analyze entire 'emma' group)\n",
        "if \"emma\" in os.listdir(BASE_DIR):\n",
        "    df_emma, slopes_emma = analyze_patient(\"emma\")\n",
        "else:\n",
        "    print(\"Patient group 'emma' not found under BASE_DIR.\")\n"
      ],
      "metadata": {
        "id": "TPVF7a7oPwvA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}